{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear and Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group members: Shirui Chen, Zihao Chen, Yuxin He"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Total Variance Denoising (TVD) is a noise reduction technique developed to denoise while preserving sharp edges in the signal. It can be achieved by taking the total variance of a signal and minimizing the cost function. Since the cost function for the TVD problem is not differentiable, iterative algorithms will have to be used. Majorization-minimization (MM) will be used as the iterative method for solving the minimized TVD cost function problem. \n",
    "\n",
    "The intent of this project is to introduce the total variation method and majorization-minimization. After participating in this project, users should be able to perform the following:\n",
    "\n",
    "- Apply total variation to a desired matrix\n",
    "- Understand the concepts of Total Variance Denoising using majorization-minimization method.\n",
    "- Able to apply majorization-minimization towards an iterative solution for a total variance denoising problem. \n",
    "\n",
    "This project includes two demonstrations of how TVD can be utilized: 1D signal recovery using TVD-MM and 2D image recovery using TVD and gradient descent. Through the activities, it can be verified that the TVD-MM method can recover signals in relatively small number of iterations, which makes it an intersting approach to denoising problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a measured signal $\\boldsymbol{y}=\\boldsymbol{x}+\\boldsymbol{w}$, where $\\boldsymbol{x}$ is a the true signal, and $\\boldsymbol{w}$ is the random noise in our measured signal. Total variance denoising reconstructs the signal through the cost function\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin} \\argmin\\limits_{\\boldsymbol{x}}\\left|\\boldsymbol{x}-\\boldsymbol{y}\\right|_2^2+\\lambda\\left|\\nabla \\boldsymbol{x}\\right|_1$\n",
    "\n",
    "The cost function is similar to cost functions covered in class. The first term is just the squared error of the difference between our measured signal and the reconstructed signal.\n",
    "\n",
    "The second term is where the total variance comes in. In the real world, for communication using electricity, digital signals tend to be piecewise constant as they tend to transmit binary information, ones and zeros, which are depicted by constant voltage or constant no voltage/negative voltage, not smoothly varying. This means that the  $l2$ norm of the signal vector doesn't make sense as a regularizer for denoising digital signals, as that would just encourage a low signal. An $l1$ norm makes more sense, as it encourages sparsity which is more constant, but we don't want a signal that is mostly zeros if the data isn't mostly zeros.\n",
    "\n",
    "A good idea would be to minimize the derivative of the signal, that way we are keeping the signal at the intensity it was, but trying to keep he signal variance low. An $l2$ norm would make sense for this task as it will keep the signal smooth, but as stated above, digital signals tend to be piecewise constant, and the transition between constant areas is not smooth. This makes the $l1$ norm of the signal derivative the ideal choice, as it encourages sparsity in the signal derivative, which means that the signal should be constant with sharp jumps. Writing out this cost function for a discretized signal, we get\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin} \\argmin\\limits_{x}\\sum_{n=0}^{N-1}\\left|x(n)-y(n)\\right|^2 + \\lambda \\sum_{n=1}^{N-1}\\left|x(n)-x(n-1)\\right|$\n",
    "\n",
    "The $\\lambda$ in the above equation is the regularization parameter, which controls the smothing of the result. Relatively lower $\\lambda$ values typically yields better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Variation (TV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $\\sum_{n=1}^{N-1}\\left|x(n)-x(n-1)\\right|$ can be represented as a vector operation on the vector $\\boldsymbol{x}$. To do this we first define the left differential operator, $\\textbf{D}$. This operator is represented by \n",
    "\n",
    "$\\textbf{D} = \\begin{bmatrix}-1 & 1 & 0 & 0 & \\dots & 0 & 0 \\\\ 0 & -1 & 1 & 0 & \\dots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & 0 & \\dots & -1 & 1 \\end{bmatrix}$\n",
    "\n",
    "There is also a matrix $\\textbf{S}$ which is the inverse of $\\textbf{D}$, which sums up all of the previous elements.\n",
    "\n",
    "This matrix has the form\n",
    "\n",
    "$\\textbf{S} = \\begin{bmatrix}0 \\\\ 1 & 0 \\\\ 1 & 1 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots \\\\ 1 & 1 & 1 & \\dots & 1 & 1 \\end{bmatrix}$\n",
    "\n",
    "And it is clear that $\\textbf{DS}=\\textbf{I}$ if you multiply them out\n",
    "\n",
    "This term is then given by $\\left|\\textbf{D} \\boldsymbol{x}\\right|_1$ , which is the total variation of signal x. Assuming x is a one-dimensional signal, as a simpler case, it can be observed that every row of matrix d only form non-zero products with x at a certain discrete sample point and the next discrete sample point. After the Dx operation, assuming x has the size of N by 1, the resulting vector is a N-1 vector of the variences of x from point to point. Because this method is able to captures the differences of the signal x from point to point, the sum of these difference would be the total varience of signal x or in a more succinct form, $\\left|\\textbf{D} \\boldsymbol{x}\\right|_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majorization-Minimization (MM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foundation of MM originated from Ortega and Rheinboldt in 1970, which was introduced as the majorization-minimization principle. It has been used in various applications since. The many bennifits of using MM includes:\n",
    "\n",
    "- It can negate the need of matrix inversion\n",
    "- It can separate the parameters of a problem\n",
    "- It can linearize an optimization problem\n",
    "- It allows an iterative method in solving a non-differenctiable problem.\n",
    "\n",
    "The MM uses a surrogate function to majorize or minorize the objective function. It can operate as a majorizer to minimize the objective function or it can operate as a minorizer to maximize the objective function. In this project, we will focus on the majorization maximization. \n",
    "\n",
    "Naming a cost function f(x) and the surrogate function g(x,y). The x and y signifies the spatial location of function g.  Two conditions must exist for the surrogate function:\n",
    "\n",
    "- Dominancy: g(x,$\\kappa$) $\\geq$ f(x) \n",
    "All points of g(x,y) are above f(x), with the exception of point $\\kappa$, which intersects f(x).\n",
    "\n",
    "- Tangency: g($\\kappa$,$\\kappa$) $\\geq$ f(x)\n",
    "The slope at where g(x,y) intersects f(x) are the same for both functions. \n",
    "\n",
    "![alt text](image2.png)\n",
    "\n",
    "\n",
    "Under these two conditions, the iterative process is as follows:\n",
    "\n",
    "#### I. Using a pre-defined second-orderg polynomial as g(x,y). Identify an $x_0$ as starting point on f(x). f($x_0$) is where g(x,$x_0$) intercepts. \n",
    "\n",
    "#### II. Use the slope of g(x,$x_0$) at $x_0$ to find the minima of g(x,$x_0$). The x-coordinate of that minima will be $x_1$.\n",
    "\n",
    "#### III. Identify f($x_1$). Use the slope of g(x,$x_1$) at $x_1$ to find the minima of g(x,$x_1$). The x-coordinate of that minima will be $x_2$. \n",
    "\n",
    "The steps continue until a predefined iteration number is reached or when the surrogate reached global minima, which is defined by the user. The figure below demonstrates the iterative steps towards the solution for minimized f(x).\n",
    "\n",
    "![alt text](image1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying MM to TVD\n",
    "\n",
    "Let the majorizing function g( t ) takes the form of $\\dfrac{1}{2|t_k|} t^2 + \\dfrac{1}{2} |t_k| \\geq |t|$ . \n",
    "\n",
    "Using v(n) in place of t:\n",
    "\n",
    "$\\sum_{n}[\\dfrac{1}{2|v_k(n)|} v^2 (n) + \\dfrac{1}{2} |v_k(n)|] \\geq \\sum_{n}|v(n)|$\n",
    "\n",
    "\n",
    "Define $\\Lambda_k$ as a diagonal matrix of v(n)\n",
    "\n",
    "$\\Lambda_k = \\begin{bmatrix} |v_k (1)| & 0 & 0 & \\dots & 0 & 0 \\\\ 0 & |v_k (2)| & 0 & \\dots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & 0 & \\dots & |v_k (n)| \\end{bmatrix} = diag(|v_k|)$\n",
    "\n",
    "The function can be re-written as: \n",
    "\n",
    "$\\dfrac{1}{2} v^T \\Lambda_k^{-1} v + \\dfrac{1}{2} ||v_k||_1 \\geq ||v||_1$\n",
    "\n",
    "Substitute Dx for v, and $\\Lambda_k$ = diag(|$Dx_k$|) \n",
    "\n",
    "The surrogate function can be written as: \n",
    "\n",
    "$\\dfrac{1}{2} x^T D^T \\Lambda_k^{-1} Dx + \\dfrac{1}{2} ||Dx_k||_1 \\geq ||Dx||_1$\n",
    "\n",
    "Multiply the a lambda as a tunable parameter:\n",
    "\n",
    "$\\lambda \\dfrac{1}{2} x^T D^T \\Lambda_k^{-1} Dx + \\lambda\\dfrac{1}{2} ||Dx_k||_1 \\geq \\lambda||Dx||_1$\n",
    "\n",
    "By adding the squared error term on both sides:\n",
    "\n",
    "$\\dfrac{1}{2}||y-x||^2_2 + \\lambda \\dfrac{1}{2} x^T D^T \\Lambda_k^{-1} Dx + \\lambda \\dfrac{1}{2} ||Dx_k||_1 \\geq \\dfrac{1}{2}||y-x||^2_2 + \\lambda||Dx||_1$\n",
    "\n",
    "It is apparent that the function on the right hand side is the cost function. Therefore, the equation on the left hand side would be the majorizer $G_k(x)$ :\n",
    "\n",
    "$G_k(x) = \\dfrac{1}{2}||y-x||^2_2 + \\lambda \\dfrac{1}{2} x^T D^T \\Lambda_k^{-1} Dx + \\lambda \\dfrac{1}{2} ||Dx_k||_1 $\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}\\argmin\\limits_{x} G_k(x)  = \\argmin\\limits_{x} \\dfrac{1}{2}||y-x||^2_2 + \\lambda \\dfrac{1}{2} x^T D^T \\Lambda_k^{-1} Dx + \\lambda \\dfrac{1}{2} ||Dx_k||_1 $\n",
    "\n",
    "#### 1) Show that the explicit solution to the majorizer is \n",
    "\n",
    "$x_{k+1} = (I + \\lambda D^T \\Lambda_k^{-1}D)^{-1} y$\n",
    "\n",
    "hint: keep in mind that $x_k$ is different from $x$ and that you are differentiating with respect to $x$ to find the minima.\n",
    "\n",
    "#### 2) Then use the matrix inverse lemma below to show that this can be rewritten as\n",
    "\n",
    "$x_{k+1} = [I - D^T (\\dfrac{1}{\\lambda} \\Lambda_k + DD^T)^{-1}D] y$\n",
    "\n",
    "lemma: $(A+BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1}-D A^{-1}B)^{-1}D A^{-1}$\n",
    "\n",
    "Recall that $\\Lambda_k$ = diag(|$Dx_k$|)\n",
    "\n",
    "The majorizor update equation can be written as:\n",
    "\n",
    "$x_{k+1} = y - D^T (\\dfrac{1}{\\lambda} diag(|Dx_k|) + DD^T)^{-1}D y$\n",
    "\n",
    "With this updated equation, it is ready to be applied to the activity provided in the next section: the 1D signal denoise application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1)\n",
    "\n",
    "#### Show that the explicit solution to the majorizer is \n",
    "\n",
    "$x_{k+1} = (I + \\lambda D^T \\Lambda_k^{-1}D)^{-1} y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n",
    "\n",
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "$\\dfrac{1}{2}||y-x||^2_2 + \\lambda \\dfrac{1}{2} x^T D^T \\Lambda_k^{-1} Dx + \\lambda \\dfrac{1}{2} ||Dx_k||_1 = \\dfrac{1}{2}(x-y)^T(x-y) + \\lambda \\dfrac{1}{2} x^T D^T \\Lambda_k^{-1} Dx + \\lambda \\dfrac{1}{2} ||Dx_k||_1 $\n",
    "\n",
    "Taking the derivative with respect to $x$ and setting it equal to zero:\n",
    "\n",
    "$0 = (x-y) + \\lambda D^T \\Lambda_k^{-1} Dx $\n",
    "\n",
    "The derivative of $||Dx_k||_1 $ is zero because $x_k$ is fixed for that iteration, and the derivative of a constant is zero.\n",
    "\n",
    "Moving y to the left size and pulling out x on the right\n",
    "\n",
    "$y = x+\\lambda D^T \\Lambda_k^{-1} Dx = (I + \\lambda D^T \\Lambda_k^{-1} D)x$\n",
    "\n",
    "We then multiply by the inverse of the stuff in the parentheses and get the answer\n",
    "\n",
    "$x_{k+1} = (I + \\lambda D^T \\Lambda_k^{-1}D)^{-1} y$\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2)\n",
    "\n",
    "##### Then use the matrix inverse lemma below to show that this can be rewritten as\n",
    "\n",
    "$x_{k+1} = [I - D^T (\\dfrac{1}{\\lambda} \\Lambda_k + DD^T)^{-1}D] y$\n",
    "\n",
    "lemma: $(A+BCD)^{-1} = A^{-1} - A^{-1}B(C^{-1}-D A^{-1}B)^{-1}D A^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Solution\n",
    "\n",
    "<details><summary> CLICK THE TRIANGLE TO VIEW </summary>\n",
    "<p>\n",
    "\n",
    "Using the lemma: $A=I$, $B=D^T$, $C = \\lambda \\Lambda_k^{-1}$, and fortunately, $D=D$\n",
    "\n",
    "Then $A^{-1}=I$ and $C^{-1}=\\dfrac{1}{\\lambda} \\Lambda_k$\n",
    "\n",
    "So $(I + \\lambda D^T \\Lambda_k^{-1}D)^{-1} = I - I D^T (\\dfrac{1}{\\lambda} \\Lambda_k - D I D^T)^{-1}D I = [I - D^T (\\dfrac{1}{\\lambda} \\Lambda_k + DD^T)^{-1}D]$\n",
    "\n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-D Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TVDMM(y,lam,Nit):\n",
    "    y = np.c_[y]\n",
    "    xold = np.zeros(y.size)\n",
    "    cost = []\n",
    "    N = len(y)\n",
    "    Id = sparse.identity(N).toarray()\n",
    "    D = Id[1:N,:] - Id[0:N-1,:]\n",
    "    DDt = np.matmul(D,np.transpose(D))\n",
    "    x = y\n",
    "    Dx = np.matmul(D,x)\n",
    "    Dy = np.matmul(D,y)\n",
    "    for i in range(Nit):\n",
    "        Empty = np.zeros((N-1,N-1))\n",
    "        np.fill_diagonal(Empty,abs(Dx)/lam)\n",
    "        F = Empty+DDt\n",
    "        x = np.subtract(y,np.matmul(np.transpose(D),np.matmul(np.linalg.inv(F),Dy)))\n",
    "        if np.linalg.norm(np.subtract(x,xold))/x.size<10e-6:\n",
    "            break\n",
    "        xold = x\n",
    "        Dx = np.matmul(D,x)\n",
    "        cost.append(.5*(sum(abs(np.square(np.subtract(x,y)))))+lam*sum(abs(Dx)))\n",
    "    return (x,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatesignal(length,nsections,seed=5):\n",
    "    # Seed random number generator\n",
    "    np.random.seed(seed)\n",
    "    boundaries = np.sort(np.random.uniform(0,length,nsections)).astype(int)\n",
    "    signal = np.zeros(length)\n",
    "    boundaries = np.unique(np.insert(boundaries,0,0))\n",
    "    boundaries = np.unique(np.insert(boundaries,boundaries.size,boundaries.size-1))\n",
    "    amplitudes = np.random.normal(0,1,boundaries.size-1)\n",
    "    for i in np.arange(1,boundaries.size):\n",
    "        signal[boundaries[i-1]:boundaries[i]] = amplitudes[i-1]\n",
    "    return signal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example electical signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = generatesignal(500,10)\n",
    "noisesignal = signal+np.random.normal(0,.2,signal.shape)\n",
    "plt.plot(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy signal compared to true signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(noisesignal)\n",
    "plt.plot(signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a recovered signal from a noisy signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvdsignal,cost = TVDMM(noisesignal,1,100)\n",
    "plt.plot(tvdsignal)\n",
    "plt.plot(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Provided Code\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generatesignal(length,nsections,seed=5):\n",
    "    # Seed random number generator\n",
    "    np.random.seed(seed)\n",
    "    boundaries = np.sort(np.random.uniform(0,length,nsections)).astype(int)\n",
    "    signal = np.zeros(length)\n",
    "    boundaries = np.unique(np.insert(boundaries,0,0))\n",
    "    boundaries = np.unique(np.insert(boundaries,boundaries.size,boundaries.size-1))\n",
    "    amplitudes = np.random.normal(0,1,boundaries.size-1)\n",
    "    for i in np.arange(1,boundaries.size):\n",
    "        signal[boundaries[i-1]:boundaries[i]] = amplitudes[i-1]\n",
    "    return signal\n",
    "\n",
    "def TVDMM(y,lam,Nit):\n",
    "    y = np.c_[y]\n",
    "    xold = np.zeros(y.size)\n",
    "    cost = []\n",
    "    N = len(y)\n",
    "    Id = sparse.identity(N).toarray()\n",
    "    D = Id[1:N,:] - Id[0:N-1,:]\n",
    "    DDt = np.matmul(D,np.transpose(D))\n",
    "    x = y\n",
    "    Dx = np.matmul(D,x)\n",
    "    Dy = np.matmul(D,y)\n",
    "    for i in range(Nit):\n",
    "        Empty = np.zeros((N-1,N-1))\n",
    "        np.fill_diagonal(Empty,abs(Dx)/lam)\n",
    "        F = Empty+DDt\n",
    "        x = np.subtract(y,np.matmul(np.transpose(D),np.matmul(np.linalg.inv(F),Dy)))\n",
    "        if np.linalg.norm(np.subtract(x,xold))/x.size<10e-6:\n",
    "            break\n",
    "        xold = x\n",
    "        Dx = np.matmul(D,x)\n",
    "        cost.append(.5*(sum(abs(np.square(np.subtract(x,y)))))+lam*sum(abs(Dx)))\n",
    "    return (x,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A) Use the provided code to generate and plot the generated signal, with noise, and the recovered signal from TVD using default values and lambda of 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hint: use np.random.normal(0,.2,signal.shape) as the effects from noise\n",
    "\n",
    "signal = generatesignal(500,10)\n",
    "\n",
    "# complete the code below:\n",
    "noisesignal = None\n",
    "# end of modification\n",
    "\n",
    "plt.plot(signal,c= 'k',label = 'Original Signal')\n",
    "plt.plot(noisesignal,alpha = .5,c = 'b', label = 'Noisy Signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>CLICK THE TRIANGLE TO VIEW (Generate Noise)</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "\n",
    "noisesignal = signal+np.random.normal(0,.2,signal.shape)\n",
    "\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: Use the function [tvdsignal,cost]=TVDMM(signal,lambda,Number of iterations) to generate \n",
    "#       the recovered TVD using default values and lambda of 0.5.\n",
    "\n",
    "# complete the code below:\n",
    "tvdsignal,cost = None\n",
    "# end of modification\n",
    "\n",
    "plt.plot(tvdsignal,c = 'r',label = 'Recovered Signal')\n",
    "plt.plot(signal,c= 'k', label = 'Original Signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>CLICK THE TRIANGLE TO VIEW  ( TVD-MM Recovery)</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "\n",
    "tvdsignal,cost = TVDMM(noisesignal,.5,100)\n",
    "\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B) What parts of the original signal are best recovered? What parts are less recovered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n",
    "\n",
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "I: The overall signal recovered nicely. The part of the signal where it is consistent recovers the best.\n",
    "\n",
    "II: The edges of the signal, where variation occurs, are not as well recovered.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C) Plot the cost function with plt.semilog(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D) Repeat part A with lambdas of 0.01, 0.1 , 0.5, and 10 and observe the effects of varying lambda. How does the cost function change? How does the recovered signal change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda = 0.01\n",
    "\n",
    "tvdsignal,cost = TVDMM(noisesignal,.01,100)\n",
    "plt.plot(tvdsignal,c = 'r',label = 'lambda = .01')\n",
    "plt.show()\n",
    "plt.semilogy(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda = 0.1\n",
    "\n",
    "tvdsignal,cost = TVDMM(noisesignal,.1,100)\n",
    "plt.plot(tvdsignal,c = 'b',label = 'lambda = .1')\n",
    "plt.show()\n",
    "plt.semilogy(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda = 0.5\n",
    "\n",
    "tvdsignal,cost = TVDMM(noisesignal,.5,100)\n",
    "plt.plot(tvdsignal,c = 'g',label = 'lambda = .5')\n",
    "plt.show()\n",
    "plt.semilogy(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda = 10\n",
    "\n",
    "tvdsignal,cost = TVDMM(noisesignal,10,100)\n",
    "plt.plot(tvdsignal,c = 'g',label = 'lambda = .5')\n",
    "plt.show()\n",
    "plt.semilogy(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n",
    "\n",
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "The higher lambda reaches convergence faster for the cost and recovers the signals better. However, with the lambda set too high, some of the subtle, sometimes critical, features would be eliminated.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E) Using a lambda of 0.4, and iterations of 1, 3, and 8, how does the number of iterations affect the recovered signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvdsignal,cost = TVDMM(noisesignal,.4,1)\n",
    "plt.plot(tvdsignal,c = 'r',label = 'lambda = .01')\n",
    "plt.show()\n",
    "\n",
    "tvdsignal,cost = TVDMM(noisesignal,.4,3)\n",
    "plt.plot(tvdsignal,c = 'b',label = 'lambda = .1')\n",
    "plt.show()\n",
    "\n",
    "tvdsignal,cost = TVDMM(noisesignal,.4,8)\n",
    "plt.plot(tvdsignal,c = 'g',label = 'lambda = .5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n",
    "\n",
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "With more iterations, the signals recovers more until the cost functin reaches convergence. Then, very little improvements will be made.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-D Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('testimage.tif')\n",
    "imarray = np.array(im)\n",
    "imarray = imarray.astype(float)\n",
    "plt.imshow(imarray,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Denoising\n",
    "\n",
    "Total Variance denoising can be performed in the 2D case, like the 1D case. Instead of a 1D gradient, a 2D gradient is used. This is a proximal gradient descent based algorithm, and the difference will be explored vs the Majorization Minimization algorithm, as well as the application of total variance denoising to natural images. We will explore the effect of different regularization parameters in total variance denoising on natural images and the convergence of the gradient descent version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TV2Ddenoising(im,lam,nit=5):\n",
    "    L2=4.0\n",
    "    tau=0.2\n",
    "    sigma=1.0/(L2*tau)\n",
    "    theta=1.0\n",
    "    lt=tau/lam\n",
    "    imshape = im.shape\n",
    "    unew = np.zeros(imshape)\n",
    "    p = np.zeros((2,)+imshape)\n",
    "    d = np.zeros(imshape)\n",
    "    ux = np.zeros(imshape)\n",
    "    uy = np.zeros(imshape)\n",
    "    mx = np.amax(im)\n",
    "    if mx>1.0:\n",
    "        im = im/mx\n",
    "    u = im\n",
    "    p[0,...] = np.subtract(np.insert(u[:,1:],imshape[1]-1,u[:,-1],axis=1),u)\n",
    "    p[1,...] = np.subtract(np.insert(u[1:,:],imshape[0]-1,u[-1,:],axis=0),u)\n",
    "    for i in np.arange(nit):\n",
    "        ux = np.subtract(np.insert(u[:,1:],imshape[1]-1,u[:,-1],axis=1),u)\n",
    "        uy = np.subtract(np.insert(u[1:,:],imshape[0]-1,u[-1,:],axis=0),u)\n",
    "        #plt.imshow(ux,vmin=0,vmax=1)\n",
    "        #plt.show()\n",
    "        #plt.imshow(uy,vmin=0,vmax=1)\n",
    "        #plt.show()\n",
    "        p=p+sigma*np.stack((ux,uy))\n",
    "        psumsquare = np.sqrt(np.add(np.square(p[0,...]),np.square(p[1,...])))\n",
    "        normep = np.amax(np.stack((psumsquare,np.ones(psumsquare.shape))),axis=0)\n",
    "        p[0,...] = np.divide(p[0,...],normep)\n",
    "        p[1,...] = np.divide(p[1,...],normep)\n",
    "        div = np.subtract(np.insert(p[1,0:-1,:],imshape[1]-1,np.zeros((1,imshape[1])),axis=0),np.insert(p[1,0:-1,:],0,np.zeros((1,imshape[1])),axis=0))\n",
    "        div = np.add(np.subtract(np.insert(p[0,:,0:-1],imshape[1]-1,np.zeros((1,imshape[1])),axis=1),np.insert(p[0,:,0:-1],0,np.zeros((1,imshape[1])),axis=1)),div)\n",
    "        v=u+tau*div\n",
    "        unew=np.multiply(v-lt,(v-im)>lt*1)+np.multiply(v+lt,(v-im)<-1*lt*1)+np.multiply(im,(np.abs(v-im)<=lt)*1)\n",
    "        u=unew+theta*np.subtract(unew,u)\n",
    "    return u*mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm works by taking the 2D gradient of the image, using that to update the image, and then applying a soft thresholding for regularization.\n",
    "\n",
    "Total variance denoising encourages piecewise constant images, which aren't always the case in real life. To test the effects of the denoising algorithm, we will use the classic cameraman image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal case\n",
    "\n",
    "In the \"ideal\" case, we will have very little image noise corrupting our image. 1) Write the code for adding noise to the image using numpy's np.add(array1,array2) function, and np.random.normal() (https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.normal.html), using zero mean noise and a standard deviation of .05 * 256, 5% of the image color depth. The size of the random array will be imarray.shape\n",
    "\n",
    "#### 1) Add in code to set noise level and crate noisy image and comment on the effect of the denoising and any loss in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "noiselvl = 0 # replace with noise level\n",
    "\n",
    "noiseim = imarray # replace with code\n",
    "\n",
    "plt.imshow(noiseim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()\n",
    "newim = TV2Ddenoising(noiseim,.75,100)\n",
    "plt.imshow(newim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n",
    "\n",
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "There is little to no loss in major features of the image, however the supports of the tripod legs do disappear.\n",
    "\n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increased noise\n",
    "\n",
    "\n",
    "Increasing the noise level changes the behavior of the denoising algorithm.\n",
    "\n",
    "Here we use a higher noise level of 20% to see the effects of the algorithm and how different lambda's effect the image appearance. To begin we will use the same lambda as above to see the effects on the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Complete the code below and repeat with higher noise level of 20% of color depth and comment on the effect of the denoising on image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: 5% of the image color depth is represented as noiselvl = .05 * 256\n",
    "\n",
    "# complete the code below:\n",
    "noiselvl = None\n",
    "# end of modification\n",
    "\n",
    "noiseim = np.add(imarray,np.random.normal(0,noiselvl,imarray.shape))\n",
    "\n",
    "plt.imshow(noiseim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()\n",
    "newim = TV2Ddenoising(noiseim,.75,100)\n",
    "plt.imshow(newim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "\n",
    "im = Image.open('cameraman.tif')\n",
    "imarray = np.array(im)\n",
    "imarray = imarray.astype(float)\n",
    "\n",
    "noiselvl = .2*256 # 20% of the image color depth\n",
    "\n",
    "noiseim = np.add(imarray,np.random.normal(0,noiselvl,imarray.shape))\n",
    "\n",
    "plt.imshow(noiseim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()\n",
    "newim = TV2Ddenoising(noiseim,.75,100)\n",
    "plt.imshow(newim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will now check the effect of over-regularizing using $\\lambda=2$ .\n",
    "\n",
    "#### 3) Copy code from above and repeat with higher lambda of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: the 2D TVD denoise code is TV2Ddenoising(noiseim, lambda, number of iterations)\n",
    "\n",
    "noiselvl = .2*256\n",
    "\n",
    "noiseim = np.add(imarray,np.random.normal(0,noiselvl,imarray.shape))\n",
    "\n",
    "plt.imshow(noiseim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()\n",
    "\n",
    "# complete the code below:\n",
    "newim = None\n",
    "# end of modification\n",
    "\n",
    "plt.imshow(newim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>CLICK THE TRIANGLE TO VIEW CODE</summary>\n",
    "<p>\n",
    "\n",
    "```\n",
    "# complete the code below:\n",
    "newim = TV2Ddenoising(noiseim,2,100)\n",
    "# end of modification\n",
    "\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "Image almost looks not-real, the number of colors in the image greatly decreases as the $l1$ norm makes the derivative sparse\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\lambda$ values too low, we can see the effects of under-regularization.\n",
    "\n",
    "#### 4) Run the code below that has a low lambda of .3 and observe the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noiselvl = .2*256\n",
    "\n",
    "noiseim = np.add(imarray,np.random.normal(0,noiselvl,imarray.shape))\n",
    "\n",
    "plt.imshow(noiseim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()\n",
    "newim = TV2Ddenoising(noiseim,.3,100)\n",
    "plt.imshow(newim,cmap='gray',vmin=0,vmax=255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "\n",
    "<details><summary>CLICK THE TRIANGLE TO VIEW</summary>\n",
    "<p>\n",
    "\n",
    "The image almost looks worse because the algorithm makes the noisy spots bigger and more apparent.\n",
    "\n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we formulated an idea for regularization that is an extension of $l1$ based least squares normalization we learned in class, instead of encouraging sparsity of the vector we are trying to denoise, we are encouraging sparsity of the derivative of our vector in Total Variance Denoising. This becomes a useful tool for denoising certain kinds of signals and images even in the case of heavy noise.\n",
    "\n",
    "However, it is not useful in all cases. Natural images tend not to be piecewise constant, and aren't a good application of TV denoising.\n",
    "\n",
    "We also introduced the Majorization-Minimization optimization algorithm which defines an approximate function greater than the actual function with a closed form solution. By using this optimization algorithm, we can cut down on the number of iterations it takes to converge compared to the gradient descent algorithm covered in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "    1. Selesnick, I. Total Variation Denoising (an MM algorithm). http://eeweb.poly.edu/iselesni/lecture_notes/TVDmm/TVDmm.pdf, January 2017\n",
    "\n",
    "    2. A. Chambolle. An algorithm for total variation minimization and applications. J. of Math. Imaging and Vision, 20:89–97, 2004\n",
    "\n",
    "    3. Giles, D. The Majorization Minimization Principle and Some Applications in Convex Optimization, 2015\n",
    "    \n",
    "    4. M. Figueiredo, J. Bioucas-Dias, and R. Nowak. Majorization-minimization algorithms for wavelet-based image restoration. IEEE Trans. Image Process., 16(12):2980–2991, December 2007.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
